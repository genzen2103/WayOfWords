{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from ebooklib import epub\n",
    "import ebooklib\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from contractions import CONTRACTION_MAP\n",
    "import unicodedata\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md', parse=True, tag=True, entity=True)\n",
    "tokenizer = ToktokTokenizer()\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "stopword_list.remove('no')\n",
    "stopword_list.remove('not')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
    "    \n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(text, remove_digits=False):\n",
    "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_stemmer(text):\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    text = ' '.join([ps.stem(word) for word in text.split()])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_corpus(corpus,contraction_expansion=True,\n",
    "                     accented_char_removal=True, text_lower_case=False, \n",
    "                     text_lemmatization=True, special_char_removal=True, \n",
    "                     stopword_removal=True, remove_digits=True):\n",
    "    progress=0\n",
    "    normalized_corpus = []\n",
    "    # normalize each document in the corpus\n",
    "    for doc in corpus:\n",
    "        # remove accented characters\n",
    "        if accented_char_removal:\n",
    "            doc = remove_accented_chars(doc)\n",
    "        # expand contractions    \n",
    "        if contraction_expansion:\n",
    "            doc = expand_contractions(doc)\n",
    "        # lowercase the text    \n",
    "        if text_lower_case:\n",
    "            doc = doc.lower()\n",
    "        # remove extra newlines\n",
    "        doc = re.sub(r'[\\r|\\n|\\r\\n]+', ' ',doc)\n",
    "        # lemmatize text\n",
    "        if text_lemmatization:\n",
    "            doc = lemmatize_text(doc)\n",
    "        # remove special characters and\\or digits    \n",
    "        if special_char_removal:\n",
    "            # insert spaces between special characters to isolate them    \n",
    "            special_char_pattern = re.compile(r'([{.(-)!}])')\n",
    "            doc = special_char_pattern.sub(\" \\\\1 \", doc)\n",
    "            doc = remove_special_characters(doc, remove_digits=remove_digits)  \n",
    "        # remove extra whitespace\n",
    "        doc = re.sub(' +', ' ', doc)\n",
    "        # remove stopwords\n",
    "        if stopword_removal:\n",
    "            doc = remove_stopwords(doc, is_lower_case=text_lower_case)\n",
    "            \n",
    "        normalized_corpus.append(doc)\n",
    "        progress+=1\n",
    "        if progress%500==0:\n",
    "            print(progress)\n",
    "        \n",
    "    return normalized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fm02 0 45\n",
      "ch01 3 129\n",
      "ch02 3 142\n",
      "ch03 3 143\n",
      "ch04 3 123\n",
      "ch05 3 113\n",
      "ch06 3 134\n",
      "ch07 3 221\n",
      "ch08 3 186\n",
      "ch09 3 236\n",
      "ch10 3 67\n",
      "ch11 1 73\n",
      "ch12 3 112\n",
      "ch13 1 67\n",
      "ch14 1 32\n",
      "ch15 1 73\n",
      "ch16 2 254\n",
      "ch17 2 118\n",
      "ch18 2 142\n",
      "ch19 2 312\n",
      "ch20 1 189\n",
      "ch21 2 210\n",
      "ch22 2 293\n",
      "ch23 2 193\n",
      "ch24 1 30\n",
      "ch25 2 183\n",
      "ch26 2 183\n",
      "ch27 2 209\n",
      "ch28 2 132\n",
      "ch29 1 114\n",
      "ch30 2 191\n",
      "ch31 2 306\n",
      "ch32 2 330\n",
      "ch33 1 88\n",
      "ch34 1 69\n",
      "ch35 1 83\n",
      "ch36 3 252\n",
      "ch37 3 121\n",
      "ch38 1 61\n",
      "ch39 3 156\n",
      "ch40 3 201\n",
      "ch41 3 74\n",
      "ch42 3 46\n",
      "ch43 3 179\n",
      "ch44 1 229\n",
      "ch45 3 63\n",
      "ch46 3 87\n",
      "ch47 3 173\n",
      "ch48 1 81\n",
      "ch49 3 205\n",
      "ch50 3 176\n",
      "ch51 1 173\n",
      "ch52 3 273\n",
      "ch53 3 254\n",
      "ch54 1 165\n",
      "ch55 3 138\n",
      "ch56 3 111\n",
      "ch57 3 52\n",
      "ch58 1 72\n",
      "ch59 1 48\n",
      "ch60 1 59\n",
      "ch61 1 79\n",
      "ch62 3 233\n",
      "ch63 3 82\n",
      "ch64 3 188\n",
      "ch65 3 192\n",
      "ch66 3 118\n",
      "ch67 3 327\n",
      "ch68 3 232\n",
      "ch69 3 229\n",
      "ch70 3 150\n",
      "ch71 3 130\n",
      "ch72 3 169\n",
      "ch73 3 81\n",
      "ch74 3 112\n",
      "ch75 3 104\n",
      "ch76 3 69\n",
      "ch77 3 287\n",
      "ch78 3 204\n",
      "ch79 3 258\n",
      "ch80 1 85\n",
      "ch81 1 86\n",
      "ch82 1 42\n",
      "ch83 1 149\n",
      "ch84 1 31\n",
      "ch85 1 70\n",
      "bm01 1 49\n",
      "end01 1 9\n",
      "bm02 0 31\n"
     ]
    }
   ],
   "source": [
    "##Way of Kings processing\n",
    "book = epub.read_epub('wok.epub')\n",
    "\n",
    "toc = book.get_item_with_id('con01')\n",
    "\n",
    "toc_pageSource = toc.get_content().decode('utf-8','ignore')\n",
    "\n",
    "soup = BeautifulSoup(toc_pageSource, 'html.parser')\n",
    "\n",
    "book1_chapters = { file['id'].replace('a',''):{'chapterName':re.sub(\".*\\: \",\"\",file.text)} for file in soup.find_all('a') if 'id' in file.attrs  }\n",
    "\n",
    "for k in ['pt01','pt02','pt03','pt04','pt05','pt06','pt07','pt08','pt09',]:\n",
    "    book1_chapters.pop(k,None)\n",
    "\n",
    "for chId in book1_chapters:\n",
    "    item = book.get_item_with_id(chId)\n",
    "    pageSource = item.get_content().decode('utf-8')\n",
    "    soup = BeautifulSoup(pageSource, 'html.parser')\n",
    "    book1_chapters[chId]['chapterHead'] = [para.text.strip()+'. ' for para in soup.find('div',attrs={'class':'chapterHead'}).find_all('p')]\n",
    "    book1_chapters[chId]['chapterBody'] = [para.text.strip()+'. ' for para in soup.find('div',attrs={'class':'chapterBody'}).find_all('p')]\n",
    "    print(chId,len(book1_chapters[chId]['chapterHead']),len(book1_chapters[chId]['chapterBody']))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prologue 2 198\n",
      "chapter1 252 252\n",
      "chapter2 266 171\n",
      "chapter3 350 86\n",
      "chapter4 157 158\n",
      "chapter5 168 294\n",
      "chapter6 177 167\n",
      "chapter7 257 117\n",
      "chapter8 292 81\n",
      "chapter9 226 125\n",
      "chapter10 2 19\n",
      "chapter11 294 171\n",
      "chapter12 306 183\n",
      "inter1-1 2 102\n",
      "inter1-2 2 129\n",
      "inter1-3 2 283\n",
      "inter1-4 2 157\n",
      "chapter13 2 118\n",
      "chapter14 2 125\n",
      "chapter15 2 85\n",
      "chapter16 2 172\n",
      "chapter17 2 195\n",
      "chapter18 2 173\n",
      "chapter19 2 59\n",
      "chapter20 2 110\n",
      "chapter21 2 96\n",
      "chapter22 2 194\n",
      "chapter23 2 75\n",
      "chapter24 2 165\n",
      "chapter25 2 180\n",
      "chapter26 2 143\n",
      "chapter27 2 33\n",
      "chapter28 2 133\n",
      "chapter29 2 106\n",
      "chapter30 2 108\n",
      "chapter31 2 104\n",
      "chapter32 2 160\n",
      "chapter33 2 134\n",
      "chapter34 2 116\n",
      "inter2-1 2 45\n",
      "inter2-2 2 23\n",
      "inter2-3 2 61\n",
      "inter2-4 2 24\n",
      "chapter35 474 145\n",
      "chapter36 570 181\n",
      "chapter37 454 98\n",
      "chapter38 315 186\n",
      "chapter39 2 121\n",
      "chapter40 306 92\n",
      "chapter41 528 262\n",
      "chapter42 377 91\n",
      "chapter43 472 158\n",
      "chapter44 441 236\n",
      "chapter45 2 251\n",
      "chapter46 393 270\n",
      "chapter47 235 120\n",
      "chapter48 2 165\n",
      "chapter49 307 237\n",
      "chapter50 474 98\n",
      "chapter51 443 99\n",
      "chapter52 476 310\n",
      "chapter53 463 118\n",
      "chapter54 255 157\n",
      "chapter55 307 261\n",
      "chapter56 339 201\n",
      "chapter57 444 154\n",
      "chapter58 430 90\n",
      "inter3-1 2 499\n",
      "inter3-2 2 31\n",
      "inter3-3 2 178\n",
      "chapter59 2 135\n",
      "chapter60 2 108\n",
      "chapter61 2 76\n",
      "chapter62 2 90\n",
      "chapter63 2 207\n",
      "chapter64 2 68\n",
      "chapter65 2 66\n",
      "chapter66 2 118\n",
      "chapter67 2 225\n",
      "chapter68 2 272\n",
      "chapter69 2 164\n",
      "chapter70 2 194\n",
      "chapter71 2 277\n",
      "chapter72 2 175\n",
      "chapter73 2 143\n",
      "chapter74 2 96\n",
      "chapter75 2 172\n",
      "inter4-1 2 80\n",
      "inter4-2 2 25\n",
      "inter4-3 2 248\n",
      "chapter76 191 223\n",
      "chapter77 313 166\n",
      "chapter78 564 135\n",
      "chapter79 375 67\n",
      "chapter80 114 125\n",
      "chapter81 314 291\n",
      "chapter82 414 156\n",
      "chapter83 374 154\n",
      "chapter84 480 139\n",
      "chapter85 266 153\n",
      "chapter86 110 267\n",
      "chapter87 459 169\n",
      "chapter88 256 180\n",
      "chapter89 514 156\n",
      "epilogue 2 80\n",
      "ars 2 50\n"
     ]
    }
   ],
   "source": [
    "##Words of Radiants processing\n",
    "book = epub.read_epub('wor.epub')\n",
    "\n",
    "toc = book.get_item_with_id('toc')\n",
    "\n",
    "toc_pageSource = toc.get_content().decode('utf-8')\n",
    "\n",
    "soup = BeautifulSoup(toc_pageSource, 'html.parser')\n",
    "\n",
    "book2_chapters = { re.sub(r'xhtml/(chapter\\d+|inter\\d+-\\d+|epilogue|ars|prologue).html.*','\\g<1>',t['src']):{ 'chapterName': re.sub('\\d+\\. (.*)','\\g<1>',t.find_parent().text ) } for t in soup.find_all('content') if re.match(r'xhtml/(chapter\\d+|inter\\d+-\\d+|epilogue|ars|prologue).html.*',t['src']) and 'image' not in t['src']}\n",
    "\n",
    "for chId in book2_chapters:\n",
    "    item = book.get_item_with_id(chId)\n",
    "    pageSource = item.get_content().decode('utf-8')\n",
    "    soup = BeautifulSoup(pageSource, 'html.parser')\n",
    "    cep  = soup.find('p',attrs={'class':'CEP'}).text.strip() if ( soup.find('p',attrs={'class':'CEP'}) ) else ''\n",
    "    cepc =  soup.find('p',attrs={'class':'CEPC'}).text.strip() if ( soup.find('p',attrs={'class':'CEP'}) ) else ''\n",
    "    book2_chapters[chId]['chapterHead'] = cep+'. '+cepc \n",
    "    co = soup.find('p',attrs={'class':'CO'}).text.strip() if ( soup.find('p',attrs={'class':'CO'}) ) else ''\n",
    "    book2_chapters[chId]['chapterBody'] =   [co+'. '] + [para.text.strip()+'. ' for para in soup.find_all('p')]\n",
    "    print(chId,len(book2_chapters[chId]['chapterHead']),len(book2_chapters[chId]['chapterBody']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45159 ['FULL LASHING: BINDING OBJECTS TOGETHER. ', 'A Full Lashing might seem very similar to a Basic Lashing, but they worked on very different principles. While one had to do with gravitation, the other had to do with the force (or Surge, as the Radiants called them) of Adhesionâ€”binding objects together as if they were one. I believe this Surge may have had something to do with atmospheric pressure.. ', 'To create a Full Lashing, a Windrunner would infuse an object with Stormlight, then press another object to it. The two objects would become bound together with an extremely powerful bond, nearly impossible to break. In fact, most materials would themselves break before the bond holding them together would.. ', 'REVERSE LASHING: GIVING AN OBJECT A GRAVITATIONAL PULL. ', 'I believe this may actually be a specialized version of the Basic Lashing. This type of Lashing required the least amount of Stormlight of any of the three Lashings. The Windrunner would infuse something, give a mental command, and create a pull to the object that yanked other objects toward it.. ', 'At its heart, this Lashing created a bubble around the object that imitated its spiritual link to the ground beneath it. As such, it was much harder for the Lashing to affect objects touching the ground, where their link to the planet was strongest. Objects falling or in flight were the easiest to influence. Other objects could be affected, but the Stormlight and skill required were much more substantial.. ', 'LIGHTWEAVING. ', 'A second form of Surgebinding involves the manipulation of light and sound in illusory tactics common throughout the cosmere. Unlike the variations present on Sel, however, this method has a powerful Spiritual element, requiring not just a full mental picture of the intended creation, but some level of connection to it as well. The illusion is based not simply upon what the Lightweaver imagines, but upon what they desire to create.. ', 'In many ways, this is the most similar ability to the original Yolish variant, which excites me. I wish to delve more into this ability, with the hope to gain a full understanding of how it relates to Cognitive and Spiritual attributes.. ']\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "for chapter in book1_chapters:\n",
    "    corpus.append(book1_chapters[chapter]['chapterName'])\n",
    "    corpus.extend(book1_chapters[chapter]['chapterHead'])\n",
    "    corpus.extend(book1_chapters[chapter]['chapterBody'])\n",
    "    \n",
    "for chapter in book2_chapters:\n",
    "    corpus.append(book2_chapters[chapter]['chapterName'])\n",
    "    corpus.extend(book2_chapters[chapter]['chapterHead'])\n",
    "    corpus.extend(book2_chapters[chapter]['chapterBody'])\n",
    "\n",
    "print(len(corpus),corpus[45150:45159])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n",
      "8000\n",
      "8500\n",
      "9000\n",
      "9500\n",
      "10000\n",
      "10500\n",
      "11000\n",
      "11500\n",
      "12000\n",
      "12500\n",
      "13000\n",
      "13500\n",
      "14000\n",
      "14500\n",
      "15000\n",
      "15500\n",
      "16000\n",
      "16500\n",
      "17000\n",
      "17500\n",
      "18000\n",
      "18500\n",
      "19000\n",
      "19500\n",
      "20000\n",
      "20500\n",
      "21000\n",
      "21500\n",
      "22000\n",
      "22500\n",
      "23000\n",
      "23500\n",
      "24000\n",
      "24500\n",
      "25000\n",
      "25500\n",
      "26000\n",
      "26500\n",
      "27000\n",
      "27500\n",
      "28000\n",
      "28500\n",
      "29000\n",
      "29500\n",
      "30000\n",
      "30500\n",
      "31000\n",
      "31500\n",
      "32000\n",
      "32500\n",
      "33000\n",
      "33500\n",
      "34000\n",
      "34500\n",
      "35000\n",
      "35500\n",
      "36000\n",
      "36500\n",
      "37000\n",
      "37500\n",
      "38000\n",
      "38500\n",
      "39000\n",
      "39500\n",
      "40000\n",
      "40500\n",
      "41000\n",
      "41500\n",
      "42000\n",
      "42500\n",
      "43000\n",
      "43500\n",
      "44000\n",
      "44500\n",
      "45000\n",
      "45159 ['full lashing binding object together', 'full lashing may seem similar basic lashing work different principle one gravitation force surge radiants call adhesionbinding object together one believe surge may something atmospheric pressure', 'create full lashing windrunner would infuse object stormlight press another object two object would become bind together extremely powerful bond nearly impossible break fact material would break bond hold together would', 'reverse lashing give object gravitational pull', 'believe may actually specialized version basic lashing type lashing require least amount stormlight three lashings windrunner would infuse something give mental command create pull object yank object toward', 'heart lashing create bubble around object imitate spiritual link ground beneath much hard lashing affect object touch ground link planet strong object fall flight easy influence object could affect stormlight skill require much substantial', 'lightweaving', 'second form surgebinding involve manipulation light sound illusory tactic common throughout cosmere unlike variation present sel however method powerful spiritual element require not full mental picture intend creation level connection well illusion base not simply upon lightweaver imago upon desire create', 'many way similar ability original yolish variant excite wish delve ability hope gain full understanding relate cognitive spiritual attribute']\n"
     ]
    }
   ],
   "source": [
    "clean_corpus = normalize_corpus(corpus)\n",
    "print(len(clean_corpus),clean_corpus[45150:45159])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|N|=45159|V|=16738\n"
     ]
    }
   ],
   "source": [
    "tokenised_sentences = [ sent.split() for sent in clean_corpus ]\n",
    "vocab = list(set(sorted([ tok for sent in tokenised_sentences for tok in sent])))\n",
    "print( \"|N|=\"+str(len(tokenised_sentences))+\"|V|=\"+str(len(vocab)) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
