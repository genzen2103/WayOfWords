{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from ebooklib import epub\n",
    "import ebooklib\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from contractions import CONTRACTION_MAP\n",
    "import unicodedata\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md', parse=True, tag=True, entity=True)\n",
    "tokenizer = ToktokTokenizer()\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "stopword_list.remove('no')\n",
    "stopword_list.remove('not')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
    "    \n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(text, remove_digits=False):\n",
    "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_stemmer(text):\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    text = ' '.join([ps.stem(word) for word in text.split()])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_corpus(corpus,contraction_expansion=True,\n",
    "                     accented_char_removal=True, text_lower_case=False, \n",
    "                     text_lemmatization=True, special_char_removal=True, \n",
    "                     stopword_removal=True, remove_digits=True):\n",
    "    \n",
    "    normalized_corpus = []\n",
    "    # normalize each document in the corpus\n",
    "    for doc in corpus:\n",
    "        # remove accented characters\n",
    "        if accented_char_removal:\n",
    "            doc = remove_accented_chars(doc)\n",
    "        # expand contractions    \n",
    "        if contraction_expansion:\n",
    "            doc = expand_contractions(doc)\n",
    "        # lowercase the text    \n",
    "        if text_lower_case:\n",
    "            doc = doc.lower()\n",
    "        # remove extra newlines\n",
    "        doc = re.sub(r'[\\r|\\n|\\r\\n]+', ' ',doc)\n",
    "        # lemmatize text\n",
    "        if text_lemmatization:\n",
    "            doc = lemmatize_text(doc)\n",
    "        # remove special characters and\\or digits    \n",
    "        if special_char_removal:\n",
    "            # insert spaces between special characters to isolate them    \n",
    "            special_char_pattern = re.compile(r'([{.(-)!}])')\n",
    "            doc = special_char_pattern.sub(\" \\\\1 \", doc)\n",
    "            doc = remove_special_characters(doc, remove_digits=remove_digits)  \n",
    "        # remove extra whitespace\n",
    "        doc = re.sub(' +', ' ', doc)\n",
    "        # remove stopwords\n",
    "        if stopword_removal:\n",
    "            doc = remove_stopwords(doc, is_lower_case=text_lower_case)\n",
    "            \n",
    "        normalized_corpus.append(doc)\n",
    "        \n",
    "    return normalized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fm02 0 45\n",
      "ch01 3 129\n",
      "ch02 3 142\n",
      "ch03 3 143\n",
      "ch04 3 123\n",
      "ch05 3 113\n",
      "ch06 3 134\n",
      "ch07 3 221\n",
      "ch08 3 186\n",
      "ch09 3 236\n",
      "ch10 3 67\n",
      "ch11 1 73\n",
      "ch12 3 112\n",
      "ch13 1 67\n",
      "ch14 1 32\n",
      "ch15 1 73\n",
      "ch16 2 254\n",
      "ch17 2 118\n",
      "ch18 2 142\n",
      "ch19 2 312\n",
      "ch20 1 189\n",
      "ch21 2 210\n",
      "ch22 2 293\n",
      "ch23 2 193\n",
      "ch24 1 30\n",
      "ch25 2 183\n",
      "ch26 2 183\n",
      "ch27 2 209\n",
      "ch28 2 132\n",
      "ch29 1 114\n",
      "ch30 2 191\n",
      "ch31 2 306\n",
      "ch32 2 330\n",
      "ch33 1 88\n",
      "ch34 1 69\n",
      "ch35 1 83\n",
      "ch36 3 252\n",
      "ch37 3 121\n",
      "ch38 1 61\n",
      "ch39 3 156\n",
      "ch40 3 201\n",
      "ch41 3 74\n",
      "ch42 3 46\n",
      "ch43 3 179\n",
      "ch44 1 229\n",
      "ch45 3 63\n",
      "ch46 3 87\n",
      "ch47 3 173\n",
      "ch48 1 81\n",
      "ch49 3 205\n",
      "ch50 3 176\n",
      "ch51 1 173\n",
      "ch52 3 273\n",
      "ch53 3 254\n",
      "ch54 1 165\n",
      "ch55 3 138\n",
      "ch56 3 111\n",
      "ch57 3 52\n",
      "ch58 1 72\n",
      "ch59 1 48\n",
      "ch60 1 59\n",
      "ch61 1 79\n",
      "ch62 3 233\n",
      "ch63 3 82\n",
      "ch64 3 188\n",
      "ch65 3 192\n",
      "ch66 3 118\n",
      "ch67 3 327\n",
      "ch68 3 232\n",
      "ch69 3 229\n",
      "ch70 3 150\n",
      "ch71 3 130\n",
      "ch72 3 169\n",
      "ch73 3 81\n",
      "ch74 3 112\n",
      "ch75 3 104\n",
      "ch76 3 69\n",
      "ch77 3 287\n",
      "ch78 3 204\n",
      "ch79 3 258\n",
      "ch80 1 85\n",
      "ch81 1 86\n",
      "ch82 1 42\n",
      "ch83 1 149\n",
      "ch84 1 31\n",
      "ch85 1 70\n",
      "bm01 1 49\n",
      "end01 1 9\n",
      "bm02 0 31\n"
     ]
    }
   ],
   "source": [
    "##Way of Kings processing\n",
    "book = epub.read_epub('wok.epub')\n",
    "\n",
    "toc = book.get_item_with_id('con01')\n",
    "\n",
    "toc_pageSource = toc.get_content().decode('utf-8')\n",
    "\n",
    "soup = BeautifulSoup(toc_pageSource, 'html.parser')\n",
    "\n",
    "book1_chapters = { file['id'].replace('a',''):{'chapterName':re.sub(\".*\\: \",\"\",file.text)} for file in soup.find_all('a') if 'id' in file.attrs  }\n",
    "\n",
    "for k in ['pt01','pt02','pt03','pt04','pt05','pt06','pt07','pt08','pt09',]:\n",
    "    book1_chapters.pop(k,None)\n",
    "\n",
    "for chId in book1_chapters:\n",
    "    item = book.get_item_with_id(chId)\n",
    "    pageSource = item.get_content().decode('utf-8')\n",
    "    soup = BeautifulSoup(pageSource, 'html.parser')\n",
    "    book1_chapters[chId]['chapterHead'] = [para.text.strip()+'. ' for para in soup.find('div',attrs={'class':'chapterHead'}).find_all('p')]\n",
    "    book1_chapters[chId]['chapterBody'] = [para.text.strip()+'. ' for para in soup.find('div',attrs={'class':'chapterBody'}).find_all('p')]\n",
    "    print(chId,len(book1_chapters[chId]['chapterHead']),len(book1_chapters[chId]['chapterBody']))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prologue 2 195\n",
      "chapter1 252 248\n",
      "chapter2 266 165\n",
      "chapter3 350 79\n",
      "chapter4 157 152\n",
      "chapter5 168 285\n",
      "chapter6 177 163\n",
      "chapter7 257 113\n",
      "chapter8 292 77\n",
      "chapter9 226 121\n",
      "chapter10 2 16\n",
      "chapter11 294 162\n",
      "chapter12 306 175\n",
      "inter1-1 2 100\n",
      "inter1-2 2 127\n",
      "inter1-3 2 279\n",
      "inter1-4 2 155\n",
      "chapter13 2 111\n",
      "chapter14 2 117\n",
      "chapter15 2 78\n",
      "chapter16 2 165\n",
      "chapter17 2 187\n",
      "chapter18 2 166\n",
      "chapter19 2 56\n",
      "chapter20 2 103\n",
      "chapter21 2 89\n",
      "chapter22 2 184\n",
      "chapter23 2 66\n",
      "chapter24 2 156\n",
      "chapter25 2 171\n",
      "chapter26 2 132\n",
      "chapter27 2 30\n",
      "chapter28 2 124\n",
      "chapter29 2 97\n",
      "chapter30 2 98\n",
      "chapter31 2 93\n",
      "chapter32 2 147\n",
      "chapter33 2 119\n",
      "chapter34 2 105\n",
      "inter2-1 2 43\n",
      "inter2-2 2 21\n",
      "inter2-3 2 58\n",
      "inter2-4 2 22\n",
      "chapter35 474 138\n",
      "chapter36 570 175\n",
      "chapter37 454 94\n",
      "chapter38 315 182\n",
      "chapter39 2 118\n",
      "chapter40 306 88\n",
      "chapter41 528 256\n",
      "chapter42 377 87\n",
      "chapter43 472 154\n",
      "chapter44 441 232\n",
      "chapter45 2 247\n",
      "chapter46 393 266\n",
      "chapter47 235 116\n",
      "chapter48 2 162\n",
      "chapter49 307 232\n",
      "chapter50 474 92\n",
      "chapter51 443 93\n",
      "chapter52 476 286\n",
      "chapter53 463 114\n",
      "chapter54 255 152\n",
      "chapter55 307 257\n",
      "chapter56 339 185\n",
      "chapter57 444 148\n",
      "chapter58 430 80\n",
      "inter3-1 2 497\n",
      "inter3-2 2 29\n",
      "inter3-3 2 176\n",
      "chapter59 2 132\n",
      "chapter60 2 104\n",
      "chapter61 2 71\n",
      "chapter62 2 85\n",
      "chapter63 2 200\n",
      "chapter64 2 63\n",
      "chapter65 2 62\n",
      "chapter66 2 115\n",
      "chapter67 2 220\n",
      "chapter68 2 269\n",
      "chapter69 2 157\n",
      "chapter70 2 186\n",
      "chapter71 2 272\n",
      "chapter72 2 170\n",
      "chapter73 2 140\n",
      "chapter74 2 93\n",
      "chapter75 2 162\n",
      "inter4-1 2 78\n",
      "inter4-2 2 23\n",
      "inter4-3 2 246\n",
      "chapter76 191 209\n",
      "chapter77 313 157\n",
      "chapter78 564 131\n",
      "chapter79 375 63\n",
      "chapter80 114 118\n",
      "chapter81 314 279\n",
      "chapter82 414 146\n",
      "chapter83 374 143\n",
      "chapter84 480 131\n",
      "chapter85 266 139\n",
      "chapter86 110 245\n",
      "chapter87 459 159\n",
      "chapter88 256 172\n",
      "chapter89 514 148\n",
      "epilogue 2 75\n",
      "endnotes 2 1\n",
      "ars 2 1\n",
      ".  ['. ']\n"
     ]
    }
   ],
   "source": [
    "##Words of Radiants processing\n",
    "book = epub.read_epub('wor.epub')\n",
    "\n",
    "toc = book.get_item_with_id('toc')\n",
    "\n",
    "toc_pageSource = toc.get_content().decode('utf-8')\n",
    "\n",
    "soup = BeautifulSoup(toc_pageSource, 'html.parser')\n",
    "\n",
    "book2_chapters = { re.sub(r'xhtml/(chapter\\d+|inter\\d+-\\d+|epilogue|ars|endnotes|prologue).html.*','\\g<1>',t['src']):{ 'chapterName': re.sub('\\d+\\. (.*)','\\g<1>',t.find_parent().text ) } for t in soup.find_all('content') if re.match(r'xhtml/(chapter\\d+|inter\\d+-\\d+|epilogue|ars|endnotes|prologue).html.*',t['src'])}\n",
    "\n",
    "for chId in book2_chapters:\n",
    "    item = book.get_item_with_id(chId)\n",
    "    \n",
    "    pageSource = item.get_content().decode('utf-8')\n",
    "    soup = BeautifulSoup(pageSource, 'html.parser')\n",
    "    cep  = soup.find('p',attrs={'class':'CEP'}).text.strip() if ( soup.find('p',attrs={'class':'CEP'}) ) else ''\n",
    "    cepc =  soup.find('p',attrs={'class':'CEPC'}).text.strip() if ( soup.find('p',attrs={'class':'CEP'}) ) else ''\n",
    "    book2_chapters[chId]['chapterHead'] = cep+'. '+cepc \n",
    "    co = soup.find('p',attrs={'class':'CO'}).text.strip() if ( soup.find('p',attrs={'class':'CO'}) ) else ''\n",
    "    book2_chapters[chId]['chapterBody'] =   [co+'. '] + [para.text.strip()+'. ' for para in soup.find_all('p',attrs={'class':'TX'})]\n",
    "    print(chId,len(book2_chapters[chId]['chapterHead']),len(book2_chapters[chId]['chapterBody']))\n",
    "\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for chapter in chapters:\n",
    "    corpus.append(chapters[chapter]['chapterName'])\n",
    "    corpus.extend(chapters[chapter]['chapterHead'])\n",
    "    corpus.extend(chapters[chapter]['chapterBody'])\n",
    "    \n",
    "clean_corpus = normalize_corpus(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "560\n",
      "The crowds thinned as they reached the upper quarter of the city, and eventually her porter pulled her to a massive building at the very apex of the city. Painted white, it was carved from the rock face itself, rather than built of bricks or clay. The pillars out front grew seamlessly from the stone, and the back side of the building melded smoothly into the cliff. The outcroppings of roof had squat domes atop them, and were painted in metallic colors. Lighteyed women passed in and out, carrying scribing utensils and wearing dresses like Shallan’s, their left hands properly cuff ed. The men entering or leaving the building wore military-style Vorin coats and stiff trousers, buttons up the sides and ending in a stiff collar that wrapped the entire neck. Many carried swords at their waists, the belts wrapping around the knee-length coats.. \n",
      "crowd thin reach upper quarter city eventually porter pull massive building apex city paint white carve rock face rather build brick clay pillar front grow seamlessly stone back side building meld smoothly cliff outcropping roof squat dome atop paint metallic color lighteyed woman pass carry scribe utensil wear dress like shallans leave hand properly cuff ed man enter leave building wear military style vorin coat stiff trouser button side end stiff collar wrap entire neck many carry sword waist belt wrap around knee length coat\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|N|=13052|V|=11962\n"
     ]
    }
   ],
   "source": [
    "tokenised_sentences = [ sent.split() for sent in clean_corpus ]\n",
    "vocab = list(set(sorted([ tok for sent in tokenised_sentences for tok in sent])))\n",
    "print( \"|N|=\"+str(len(tokenised_sentences))+\"|V|=\"+str(len(vocab)) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
