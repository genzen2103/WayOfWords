{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from ebooklib import epub\n",
    "import ebooklib\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from contractions import CONTRACTION_MAP\n",
    "import unicodedata\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md', parse=True, tag=True, entity=True)\n",
    "tokenizer = ToktokTokenizer()\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "stopword_list.remove('no')\n",
    "stopword_list.remove('not')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
    "    \n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(text, remove_digits=False):\n",
    "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_stemmer(text):\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    text = ' '.join([ps.stem(word) for word in text.split()])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_corpus(corpus,contraction_expansion=True,\n",
    "                     accented_char_removal=True, text_lower_case=False, \n",
    "                     text_lemmatization=True, special_char_removal=True, \n",
    "                     stopword_removal=True, remove_digits=True):\n",
    "    progress=0\n",
    "    normalized_corpus = []\n",
    "    # normalize each document in the corpus\n",
    "    for doc in corpus:\n",
    "        # remove accented characters\n",
    "        if accented_char_removal:\n",
    "            doc = remove_accented_chars(doc)\n",
    "        # expand contractions    \n",
    "        if contraction_expansion:\n",
    "            doc = expand_contractions(doc)\n",
    "        # lowercase the text    \n",
    "        if text_lower_case:\n",
    "            doc = doc.lower()\n",
    "        # remove extra newlines\n",
    "        doc = re.sub(r'[\\r|\\n|\\r\\n]+', ' ',doc)\n",
    "        # lemmatize text\n",
    "        if text_lemmatization:\n",
    "            doc = lemmatize_text(doc)\n",
    "        # remove special characters and\\or digits    \n",
    "        if special_char_removal:\n",
    "            # insert spaces between special characters to isolate them    \n",
    "            special_char_pattern = re.compile(r'([{.(-)!}])')\n",
    "            doc = special_char_pattern.sub(\" \\\\1 \", doc)\n",
    "            doc = remove_special_characters(doc, remove_digits=remove_digits)  \n",
    "        # remove extra whitespace\n",
    "        doc = re.sub(' +', ' ', doc)\n",
    "        # remove stopwords\n",
    "        if stopword_removal:\n",
    "            doc = remove_stopwords(doc, is_lower_case=text_lower_case)\n",
    "            \n",
    "        normalized_corpus.append(doc)\n",
    "        progress+=1\n",
    "        if progress%500==0:\n",
    "            print(progress)\n",
    "        \n",
    "    return normalized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fm02 0 45\n",
      "ch01 3 129\n",
      "ch02 3 142\n",
      "ch03 3 143\n",
      "ch04 3 123\n",
      "ch05 3 113\n",
      "ch06 3 134\n",
      "ch07 3 221\n",
      "ch08 3 186\n",
      "ch09 3 236\n",
      "ch10 3 67\n",
      "ch11 1 73\n",
      "ch12 3 112\n",
      "ch13 1 67\n",
      "ch14 1 32\n",
      "ch15 1 73\n",
      "ch16 2 254\n",
      "ch17 2 118\n",
      "ch18 2 142\n",
      "ch19 2 312\n",
      "ch20 1 189\n",
      "ch21 2 210\n",
      "ch22 2 293\n",
      "ch23 2 193\n",
      "ch24 1 30\n",
      "ch25 2 183\n",
      "ch26 2 183\n",
      "ch27 2 209\n",
      "ch28 2 132\n",
      "ch29 1 114\n",
      "ch30 2 191\n",
      "ch31 2 306\n",
      "ch32 2 330\n",
      "ch33 1 88\n",
      "ch34 1 69\n",
      "ch35 1 83\n",
      "ch36 3 252\n",
      "ch37 3 121\n",
      "ch38 1 61\n",
      "ch39 3 156\n",
      "ch40 3 201\n",
      "ch41 3 74\n",
      "ch42 3 46\n",
      "ch43 3 179\n",
      "ch44 1 229\n",
      "ch45 3 63\n",
      "ch46 3 87\n",
      "ch47 3 173\n",
      "ch48 1 81\n",
      "ch49 3 205\n",
      "ch50 3 176\n",
      "ch51 1 173\n",
      "ch52 3 273\n",
      "ch53 3 254\n",
      "ch54 1 165\n",
      "ch55 3 138\n",
      "ch56 3 111\n",
      "ch57 3 52\n",
      "ch58 1 72\n",
      "ch59 1 48\n",
      "ch60 1 59\n",
      "ch61 1 79\n",
      "ch62 3 233\n",
      "ch63 3 82\n",
      "ch64 3 188\n",
      "ch65 3 192\n",
      "ch66 3 118\n",
      "ch67 3 327\n",
      "ch68 3 232\n",
      "ch69 3 229\n",
      "ch70 3 150\n",
      "ch71 3 130\n",
      "ch72 3 169\n",
      "ch73 3 81\n",
      "ch74 3 112\n",
      "ch75 3 104\n",
      "ch76 3 69\n",
      "ch77 3 287\n",
      "ch78 3 204\n",
      "ch79 3 258\n",
      "ch80 1 85\n",
      "ch81 1 86\n",
      "ch82 1 42\n",
      "ch83 1 149\n",
      "ch84 1 31\n",
      "ch85 1 70\n",
      "bm01 1 49\n",
      "end01 1 9\n",
      "bm02 0 31\n"
     ]
    }
   ],
   "source": [
    "##Way of Kings processing\n",
    "book = epub.read_epub('wok.epub')\n",
    "\n",
    "toc = book.get_item_with_id('con01')\n",
    "\n",
    "toc_pageSource = toc.get_content().decode('utf-8','ignore')\n",
    "\n",
    "soup = BeautifulSoup(toc_pageSource, 'html.parser')\n",
    "\n",
    "book1_chapters = { file['id'].replace('a',''):{'chapterName':re.sub(\".*\\: \",\"\",file.text)} for file in soup.find_all('a') if 'id' in file.attrs  }\n",
    "\n",
    "for k in ['pt01','pt02','pt03','pt04','pt05','pt06','pt07','pt08','pt09',]:\n",
    "    book1_chapters.pop(k,None)\n",
    "\n",
    "for chId in book1_chapters:\n",
    "    item = book.get_item_with_id(chId)\n",
    "    pageSource = item.get_content().decode('utf-8')\n",
    "    soup = BeautifulSoup(pageSource, 'html.parser')\n",
    "    book1_chapters[chId]['chapterHead'] = [para.text.strip()+'. ' for para in soup.find('div',attrs={'class':'chapterHead'}).find_all('p')]\n",
    "    book1_chapters[chId]['chapterBody'] = [para.text.strip()+'. ' for para in soup.find('div',attrs={'class':'chapterBody'}).find_all('p')]\n",
    "    print(chId,len(book1_chapters[chId]['chapterHead']),len(book1_chapters[chId]['chapterBody']))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prologue 2 198\n",
      "chapter1 252 252\n",
      "chapter2 266 171\n",
      "chapter3 350 86\n",
      "chapter4 157 158\n",
      "chapter5 168 294\n",
      "chapter6 177 167\n",
      "chapter7 257 117\n",
      "chapter8 292 81\n",
      "chapter9 226 125\n",
      "chapter10 2 19\n",
      "chapter11 294 171\n",
      "chapter12 306 183\n",
      "inter1-1 2 102\n",
      "inter1-2 2 129\n",
      "inter1-3 2 283\n",
      "inter1-4 2 157\n",
      "chapter13 49 118\n",
      "chapter14 48 125\n",
      "chapter15 49 85\n",
      "chapter16 49 172\n",
      "chapter17 50 195\n",
      "chapter18 49 173\n",
      "chapter19 2 59\n",
      "chapter20 49 110\n",
      "chapter21 49 96\n",
      "chapter22 46 194\n",
      "chapter23 49 75\n",
      "chapter24 49 165\n",
      "chapter25 52 180\n",
      "chapter26 46 143\n",
      "chapter27 2 33\n",
      "chapter28 49 133\n",
      "chapter29 51 106\n",
      "chapter30 51 108\n",
      "chapter31 49 104\n",
      "chapter32 46 160\n",
      "chapter33 47 134\n",
      "chapter34 50 116\n",
      "inter2-1 2 45\n",
      "inter2-2 2 23\n",
      "inter2-3 2 61\n",
      "inter2-4 2 24\n",
      "chapter35 474 145\n",
      "chapter36 570 181\n",
      "chapter37 454 98\n",
      "chapter38 315 186\n",
      "chapter39 2 121\n",
      "chapter40 306 92\n",
      "chapter41 528 262\n",
      "chapter42 377 91\n",
      "chapter43 472 158\n",
      "chapter44 441 236\n",
      "chapter45 2 251\n",
      "chapter46 393 270\n",
      "chapter47 235 120\n",
      "chapter48 2 165\n",
      "chapter49 307 237\n",
      "chapter50 474 98\n",
      "chapter51 443 99\n",
      "chapter52 476 310\n",
      "chapter53 463 118\n",
      "chapter54 255 157\n",
      "chapter55 307 261\n",
      "chapter56 339 201\n",
      "chapter57 444 154\n",
      "chapter58 430 90\n",
      "inter3-1 2 499\n",
      "inter3-2 2 31\n",
      "inter3-3 2 178\n",
      "chapter59 2 135\n",
      "chapter60 2 108\n",
      "chapter61 2 76\n",
      "chapter62 2 90\n",
      "chapter63 2 207\n",
      "chapter64 2 68\n",
      "chapter65 2 66\n",
      "chapter66 2 118\n",
      "chapter67 2 225\n",
      "chapter68 2 272\n",
      "chapter69 2 164\n",
      "chapter70 2 194\n",
      "chapter71 2 277\n",
      "chapter72 2 175\n",
      "chapter73 2 143\n",
      "chapter74 2 96\n",
      "chapter75 2 172\n",
      "inter4-1 2 80\n",
      "inter4-2 2 25\n",
      "inter4-3 2 248\n",
      "chapter76 191 223\n",
      "chapter77 313 166\n",
      "chapter78 564 135\n",
      "chapter79 375 67\n",
      "chapter80 114 125\n",
      "chapter81 314 291\n",
      "chapter82 414 156\n",
      "chapter83 374 154\n",
      "chapter84 480 139\n",
      "chapter85 266 153\n",
      "chapter86 110 267\n",
      "chapter87 459 169\n",
      "chapter88 256 180\n",
      "chapter89 514 156\n",
      "epilogue 2 80\n",
      "ars 2 50\n"
     ]
    }
   ],
   "source": [
    "##Words of Radiants processing\n",
    "book = epub.read_epub('wor.epub')\n",
    "\n",
    "toc = book.get_item_with_id('toc')\n",
    "\n",
    "toc_pageSource = toc.get_content().decode('utf-8')\n",
    "\n",
    "soup = BeautifulSoup(toc_pageSource, 'html.parser')\n",
    "\n",
    "book2_chapters = { re.sub(r'xhtml/(chapter\\d+|inter\\d+-\\d+|epilogue|ars|prologue).html.*','\\g<1>',t['src']):{ 'chapterName': re.sub('\\d+\\. (.*)','\\g<1>',t.find_parent().text ) } for t in soup.find_all('content') if re.match(r'xhtml/(chapter\\d+|inter\\d+-\\d+|epilogue|ars|prologue).html.*',t['src']) and 'image' not in t['src']}\n",
    "\n",
    "for chId in book2_chapters:\n",
    "    item = book.get_item_with_id(chId)\n",
    "    pageSource = item.get_content().decode('utf-8')\n",
    "    soup = BeautifulSoup(pageSource, 'html.parser')\n",
    "    cep  = soup.find('p',attrs={'class':'CEP'}).text.strip() if ( soup.find('p',attrs={'class':'CEP'}) ) else ''\n",
    "    cepc =  soup.find('p',attrs={'class':'CEPC'}).text.strip() if ( soup.find('p',attrs={'class':'CEPC'}) ) else ''\n",
    "    book2_chapters[chId]['chapterHead'] = cep+'. '+cepc \n",
    "    co = soup.find('p',attrs={'class':'CO'}).text.strip() if ( soup.find('p',attrs={'class':'CO'}) ) else ''\n",
    "    book2_chapters[chId]['chapterBody'] =   [co+'. '] + [para.text.strip()+'. ' for para in soup.find_all('p')]\n",
    "    print(chId,len(book2_chapters[chId]['chapterHead']),len(book2_chapters[chId]['chapterBody']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prologue 2 101\n",
      "chapter1 158 83\n",
      "chapter2 56 112\n",
      "chapter3 2 139\n",
      "chapter4 145 178\n",
      "chapter5 191 78\n",
      "chapter6 126 85\n",
      "chapter7 42 194\n",
      "chapter8 124 118\n",
      "chapter9 96 49\n",
      "chapter10 116 170\n",
      "chapter11 2 169\n",
      "chapter12 88 202\n",
      "chapter13 78 146\n",
      "chapter14 161 36\n",
      "chapter15 139 118\n",
      "chapter16 57 232\n",
      "chapter17 148 115\n",
      "chapter18 101 254\n",
      "chapter19 2 145\n",
      "chapter20 154 54\n",
      "chapter21 87 152\n",
      "chapter22 89 86\n",
      "chapter23 79 63\n",
      "chapter24 161 139\n",
      "chapter25 124 135\n",
      "chapter26 2 166\n",
      "chapter27 140 126\n",
      "chapter28 171 177\n",
      "chapter29 99 237\n",
      "chapter30 58 37\n",
      "chapter31 98 185\n",
      "chapter32 63 38\n",
      "int1 2 25\n",
      "int2 2 71\n",
      "int3 2 68\n",
      "chapter33 2 67\n",
      "chapter34 2 165\n",
      "chapter35 2 157\n",
      "chapter36 2 79\n",
      "chapter37 2 259\n",
      "chapter38 2 219\n",
      "ill9 2 2\n",
      "chapter39 2 137\n",
      "chapter40 2 133\n",
      "chapter41 2 58\n",
      "chapter42 2 92\n",
      "chapter43 2 77\n",
      "chapter44 2 164\n",
      "chapter45 2 76\n",
      "chapter46 2 153\n",
      "chapter47 2 71\n",
      "chapter48 2 61\n",
      "chapter49 2 178\n",
      "chapter50 2 139\n",
      "chapter51 2 78\n",
      "chapter52 2 102\n",
      "ill11 2 2\n",
      "chapter53 2 143\n",
      "chapter54 2 116\n",
      "chapter55 2 104\n",
      "chapter56 2 77\n",
      "chapter57 2 98\n",
      "int4 2 108\n",
      "int5 2 120\n",
      "int6 2 89\n",
      "chapter58 154 70\n",
      "chapter59 217 213\n",
      "chapter60 73 76\n",
      "chapter61 163 162\n",
      "chapter62 142 145\n",
      "chapter63 222 102\n",
      "chapter64 200 117\n",
      "chapter65 118 188\n",
      "chapter66 2 94\n",
      "ill14 2 2\n",
      "chapter67 203 147\n",
      "chapter68 224 161\n",
      "chapter69 209 112\n",
      "chapter70 198 96\n",
      "chapter71 2 126\n",
      "chapter72 175 149\n",
      "chapter73 131 127\n",
      "chapter74 224 106\n",
      "chapter75 2 99\n",
      "chapter76 2 131\n",
      "ill15 2 2\n",
      "chapter77 203 172\n",
      "chapter78 227 142\n",
      "chapter79 182 70\n",
      "chapter80 211 61\n",
      "chapter81 227 36\n",
      "chapter82 115 153\n",
      "chapter83 189 168\n",
      "chapter84 200 178\n",
      "chapter85 116 50\n",
      "chapter86 257 54\n",
      "chapter87 101 24\n",
      "int7 2 67\n",
      "int8 2 39\n",
      "int9 2 11\n",
      "int10 2 36\n",
      "int11 2 45\n",
      "chapter88 2 77\n",
      "ill16 2 2\n",
      "chapter89 240 139\n",
      "chapter90 266 66\n",
      "chapter91 261 89\n",
      "chapter92 363 147\n",
      "chapter93 181 86\n",
      "ill17 2 2\n",
      "chapter94 2 32\n",
      "chapter95 268 57\n",
      "chapter96 144 145\n",
      "chapter97 192 161\n",
      "chapter98 221 85\n",
      "chapter99 242 151\n",
      "chapter100 225 134\n",
      "chapter101 260 91\n",
      "chapter102 204 189\n",
      "chapter103 468 95\n",
      "chapter104 348 97\n",
      "chapter105 2 107\n",
      "chapter106 299 58\n",
      "chapter107 210 227\n",
      "ill20 2 2\n",
      "chapter108 258 242\n",
      "chapter109 133 151\n",
      "chapter110 206 65\n",
      "chapter111 227 175\n",
      "chapter112 224 59\n",
      "chapter113 231 72\n",
      "int12 2 28\n",
      "int13 2 165\n",
      "int14 2 34\n",
      "chapter114 2 149\n",
      "chapter115 197 167\n",
      "chapter116 227 206\n",
      "chapter117 297 241\n",
      "chapter118 219 209\n",
      "chapter119 187 194\n",
      "chapter120 179 719\n",
      "chapter121 130 366\n",
      "chapter122 88 289\n",
      "epilogue 2 74\n",
      "backmatter 2 57\n"
     ]
    }
   ],
   "source": [
    "##Oathbringer processing\n",
    "book = epub.read_epub('oath.epub')\n",
    "\n",
    "toc = book.get_item_with_id('toc')\n",
    "\n",
    "toc_pageSource = toc.get_content().decode('utf-8')\n",
    "\n",
    "soup = BeautifulSoup(toc_pageSource, 'html.parser')\n",
    "\n",
    "book3_chapters = { re.sub(r'xhtml/(chapter\\d+|int\\d+|epilogue|ars|prologue|ill11|ill20|ill22|ill16|ill9|ill17|ill14|ill15|ill18|backmatter).xhtml.*','\\g<1>',t.find('content')['src']): { 'chapterName' : re.sub( '.*\\. (.*)','\\g<1>', t.find('text').text )}  for t in soup.find_all('navpoint') if re.match(r'xhtml/(chapter\\d+|int\\d+|epilogue|ars|prologue|ill11|ill20|ill16|ill9|ill17|ill14|ill15|ill22«ill18|backmatter).xhtml.*',t.find('content')['src']) and 'image' not in t.find('content')['src']}\n",
    "\n",
    "for chId in book3_chapters:\n",
    "    item = book.get_item_with_id(chId)\n",
    "    pageSource = item.get_content().decode('utf-8')\n",
    "    soup = BeautifulSoup(pageSource, 'html.parser')\n",
    "    cep  = soup.find('p',attrs={'class':'EP'}).text.strip() if ( soup.find('p',attrs={'class':'EP'}) ) else ''\n",
    "    cepc = soup.find('p',attrs={'class':'EPC'}).text.strip() if ( soup.find('p',attrs={'class':'EPC'}) ) else ''\n",
    "    co = soup.find('p',attrs={'class':'CO'}).text.strip() if ( soup.find('p',attrs={'class':'CO'}) ) else ''\n",
    "    \n",
    "    book3_chapters[chId]['chapterBody'] =   [ co+'. ' ] + [ para.text.strip()+'. ' for para in soup.find_all('p') ]\n",
    "    book3_chapters[chId]['chapterHead'] = cep+'. '+cepc \n",
    "    \n",
    "    print(chId,len(book3_chapters[chId]['chapterHead']),len(book3_chapters[chId]['chapterBody']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78828 ['A second form of Surgebinding involves the manipulation of light and sound in illusory tactics common throughout the cosmere. Unlike the variations present on Sel, however, this method has a powerful Spiritual element, requiring not just a full mental picture of the intended creation, but some level of Connection to it as well. The illusion is based not simply upon what the Lightweaver imagines, but upon what they desire to create.. ', 'In many ways, this is the most similar ability to the original Yolish variant, which excites me. I wish to delve more into this ability, with the hope to gain a full understanding of how it relates to cognitive and spiritual attributes.. ', 'SOULCASTING. ', 'Essential to the economy of Roshar is the art of Soulcasting, in which one form of matter is directly transformed into another by changing its spiritual nature. This is performed on Roshar via the use of devices known as Soulcasters, and these devices the majority of which appear to be focused on turning stone into grain or flesh are used to provide mobile supply for armies or to augment local urban food stores. This has allowed kingdoms on Roshar—where fresh water is rarely an issue, because of highstorm rains—to field armies in ways that would be unthinkable elsewhere.. ', 'What intrigues me most about Soulcasting, however, are the things we can infer about the world and Investiture from it. For example, certain gemstones are requisite in producing certain results—if you wish to produce grain, however, your Soulcaster must both be attuned to that transformation and have an emerald not a different gemstone attached. This creates an economy based on the relative values of what the gemstones can create, not upon their rarity. Indeed, as the chemical structures are identical for several of these gemstone varieties, aside from trace impurities, the color is the most important part—not their actual axial makeup. I’m certain you will find this relevance of hue quite intriguing, particularly in its relationship to other forms of Investiture.. ', 'This relationship must have been essential in the local creation of the table I’ve included above, which lacks some scientific merit, but is intrinsically tied to the folklore surrounding Soulcasting. An emerald can be used to create food—and thus is traditionally associated with a similar Essence. Indeed, on Roshar there are considered to be ten elements; not the traditional four or sixteen, depending upon local tradition.. ', 'Curiously, these gemstones seem tied to the original abilities of the Soulcasters who were an order of Knights Radiant—but they don’t seem essential to the actual operation of the Investiture when performed by a living Radiant. I do not know the connection here, though it implies something valuable.. ', 'Soulcasters, the devices, were created to imitate the abilities of the Surge of Soulcasting or Transformation. This is yet another mechanical imitation of something once only available only to a select few within the bounds of an Invested Art. The Honorblades on Roshar, indeed, may be the very first example of this—from thousands of years ago. I believe this has relevance to the discoveries being made on Scadrial, and the commoditization of Allomancy and Feruchemy.. ']\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "for chapter in book1_chapters:\n",
    "    corpus.append(book1_chapters[chapter]['chapterName'])\n",
    "    corpus.extend(book1_chapters[chapter]['chapterHead'])\n",
    "    corpus.extend(book1_chapters[chapter]['chapterBody'])\n",
    "    \n",
    "for chapter in book2_chapters:\n",
    "    corpus.append(book2_chapters[chapter]['chapterName'])\n",
    "    corpus.extend(book2_chapters[chapter]['chapterHead'])\n",
    "    corpus.extend(book2_chapters[chapter]['chapterBody'])\n",
    "\n",
    "for chapter in book3_chapters:\n",
    "    corpus.append(book3_chapters[chapter]['chapterName'])\n",
    "    corpus.extend(book3_chapters[chapter]['chapterHead'])\n",
    "    corpus.extend(book3_chapters[chapter]['chapterBody'])\n",
    "    \n",
    "\n",
    "\n",
    "less_clean_corpus = list( map( lambda sent: sent.replace('\\u200b', '')\n",
    "                                            .replace('\\xa0','')\n",
    "                                            .replace('ç','')\n",
    "                                            .replace('\\n', '')\n",
    "                                            .replace('•', '')\n",
    "                                            .replace('…', '')\n",
    "                                            .replace('/','')\n",
    "                                            .replace('(','')\n",
    "                                            .replace(')','')\n",
    "                                            .replace(' ', ' ') , corpus) )\n",
    "\n",
    "print(len(less_clean_corpus),less_clean_corpus[78820:78828])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7183548\n"
     ]
    }
   ],
   "source": [
    "char_corpus = [c for sent in less_clean_corpus for c in sent]\n",
    "\n",
    "print(len(char_corpus))\n",
    "\n",
    "char_to_num = { char:i for i,char in  enumerate(sorted(set(char_corpus))) }\n",
    "\n",
    "num_to_char = dict(zip(char_to_num.values(),char_to_num.keys()))\n",
    "\n",
    "SEQ_LENGTH = 50\n",
    "\n",
    "VOCAB_SIZE = len(char_to_num.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((int(len(char_corpus)/SEQ_LENGTH), SEQ_LENGTH, VOCAB_SIZE))\n",
    "y = np.zeros((int(len(char_corpus)/SEQ_LENGTH), SEQ_LENGTH, VOCAB_SIZE))\n",
    "for i in range(0, int( len(char_corpus)/SEQ_LENGTH) ):\n",
    "    X_sequence = char_corpus[i*SEQ_LENGTH:(i+1)*SEQ_LENGTH]\n",
    "    X_sequence_ix = [char_to_num[value] for value in X_sequence]\n",
    "    input_sequence = np.zeros((SEQ_LENGTH, VOCAB_SIZE))\n",
    "    for j in range(SEQ_LENGTH):\n",
    "        input_sequence[j][X_sequence_ix[j]] = 1.\n",
    "    X[i] = input_sequence\n",
    "\n",
    "    y_sequence = char_corpus[i*SEQ_LENGTH+1:(i+1)*SEQ_LENGTH+1]\n",
    "    y_sequence_ix = [char_to_num[value] for value in y_sequence]\n",
    "    target_sequence = np.zeros((SEQ_LENGTH, VOCAB_SIZE))\n",
    "    for j in range(SEQ_LENGTH):\n",
    "        target_sequence[j][y_sequence_ix[j]] = 1.\n",
    "    y[i] = target_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(143670, 50, 77)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/Cellar/python/3.7.0/Frameworks/Python.framework/Versions/3.7/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.7\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.recurrent import LSTM, SimpleRNN\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "\n",
    "HIDDEN_DIM = 300\n",
    "LAYER_NUM = 2\n",
    "GENERATE_LENGTH = 1000\n",
    "BATCH_SIZE = 200\n",
    "MAX_EPOCHS = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(HIDDEN_DIM, input_shape=(None, VOCAB_SIZE), return_sequences=True))\n",
    "for i in range(LAYER_NUM - 1):\n",
    "    model.add(LSTM(HIDDEN_DIM, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(VOCAB_SIZE)))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")\n",
    "\n",
    "\n",
    "def generate_text(model, length):\n",
    "    ix = [np.random.randint(VOCAB_SIZE)]\n",
    "    y_char = [num_to_char[ix[-1]]]\n",
    "    X = np.zeros((1, length, VOCAB_SIZE))\n",
    "    for i in range(length):\n",
    "        X[0, i, :][ix[-1]] = 1\n",
    "        print(num_to_char[ix[-1]], end=\"\")\n",
    "        ix = np.argmax(model.predict(X[:, :i+1, :])[0], 1)\n",
    "        y_char.append(num_to_char[ix[-1]])\n",
    "    return ('').join(y_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Epoch 1/1\n",
      "143670/143670 [==============================] - 2326s 16ms/step - loss: 2.1977\n",
      "Red to the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the storm the\n",
      "\n",
      "\n",
      "Epoch 1/1\n",
      "143670/143670 [==============================] - 916s 6ms/step - loss: 1.5015\n",
      "For the world was a soldier was a soldier was a soldier was a soldier was a soldier was a soldier was a soldier was a soldier was a soldier was a soldier was a soldier was a soldier was a soldier was a soldier was a soldier was a soldier was a soldier was a soldier was a soldier was a soldier was a soldier was a soldier was a soldier was a soldier was a soldier was a soldier was a soldier was a soldier was a soldier was a soldier was a soldier was a soldier was a soldier was a soldier was a soldier was a soldier was a soldier was a soldier was a soldier was a soldier was a soldier was a soldier was a soldier was a soldier was a soldier was a soldier was a soldier was a soldier was a soldier was a soldier was a soldier was a soldier was a soldier was a soldier was a soldier was a soldier was a soldier was a soldier was a soldier was a soldier was a soldier was a soldier was a soldier was a soldier was a soldier was a soldier was a soldier was a soldier was a soldier was a soldier was a \n",
      "\n",
      "\n",
      "Epoch 1/1\n",
      "143670/143670 [==============================] - 2966s 21ms/step - loss: 1.3378\n",
      "and the storm of the storm of the storm was a thing to be a strange of the storm. The storm was a thing to be a strange of the storm. The storm was a thing to be a strange of the storm. The storm was a thing to be a strange of the storm. The storm was a thing to be a strange of the storm. The storm was a thing to be a strange of the storm. The storm was a thing to be a strange of the storm. The storm was a thing to be a strange of the storm. The storm was a thing to be a strange of the storm. The storm was a thing to be a strange of the storm. The storm was a thing to be a strange of the storm. The storm was a thing to be a strange of the storm. The storm was a thing to be a strange of the storm. The storm was a thing to be a strange of the storm. The storm was a thing to be a strange of the storm. The storm was a thing to be a strange of the storm. The storm was a thing to be a strange of the storm. The storm was a thing to be a strange of the storm. The storm was a thing to be a stra\n",
      "\n",
      "\n",
      "Epoch 1/1\n",
      "143670/143670 [==============================] - 7471s 52ms/step - loss: 1.2689\n",
      "n the storms and the storms and the storms and the storms and the storms and the storms and the storms and the storms and the storms and the storms and the storms and the storms and the storms and the storms and the storms and the storms and the storms and the storms and the storms and the storms and the storms and the storms and the storms and the sto"
     ]
    }
   ],
   "source": [
    "nb_epoch = 0\n",
    "while nb_epoch<MAX_EPOCHS:\n",
    "    print('\\n\\n')\n",
    "    model.fit(X, y, batch_size=BATCH_SIZE, verbose=1, epochs=1)\n",
    "    nb_epoch += 1\n",
    "    generate_text(model, GENERATE_LENGTH)\n",
    "    if nb_epoch % 10 == 0:\n",
    "        model.save_weights('checkpoint_{}_epoch_{}.hdf5'.format(HIDDEN_DIM, nb_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
